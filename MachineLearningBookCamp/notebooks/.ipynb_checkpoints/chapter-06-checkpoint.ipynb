{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "91cf45a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install scikit-learn\n",
    "\n",
    "import pandas\n",
    "import numpy\n",
    "import seaborn\n",
    "from matplotlib import pyplot\n",
    "#matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "54909317",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = pandas.read_csv(\"/mlbc/chapter-06-credit-risk/CreditScoring.csv\")\n",
    "dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "662d3368",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe.columns = dataframe.columns.str.lower()\n",
    "dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "48d4c157",
   "metadata": {},
   "outputs": [],
   "source": [
    "status_values = {\n",
    "    1: \"ok\",\n",
    "    2: \"default\",\n",
    "    0: \"unk\"\n",
    "}\n",
    "\n",
    "dataframe.status = dataframe.status.map(status_values)\n",
    "dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2138601a",
   "metadata": {},
   "outputs": [],
   "source": [
    "home_values = {\n",
    "    1: \"rent\",\n",
    "    2: \"owner\",\n",
    "    3: \"private\",\n",
    "    4: \"ignored\",\n",
    "    5: \"parents\",\n",
    "    6: \"other\",\n",
    "    0: \"unk\"\n",
    "}\n",
    "\n",
    "dataframe.home = dataframe.home.map(home_values)\n",
    "\n",
    "marital_values = {\n",
    "    1: \"single\",\n",
    "    2: \"married\",\n",
    "    3: \"widow\",\n",
    "    4: \"separated\",\n",
    "    5: \"divorced\",\n",
    "    0: \"unk\"\n",
    "}\n",
    "\n",
    "dataframe.marital = dataframe.marital.map(marital_values)\n",
    "\n",
    "records_values = {\n",
    "    1: \"no\",\n",
    "    2: \"yes\",\n",
    "    0: \"unk\"\n",
    "}\n",
    "\n",
    "dataframe.records = dataframe.records.map(records_values)\n",
    "\n",
    "job_values = {\n",
    "    1: \"fixed\",\n",
    "    2: \"parttime\",\n",
    "    3: \"freelance\",\n",
    "    4: \"others\",\n",
    "    0: \"unk\"\n",
    "}\n",
    "\n",
    "dataframe.job = dataframe.job.map(job_values)\n",
    "\n",
    "dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "414cde51",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataframe.describe().round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "40b44781",
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in [\"income\", \"assets\", \"debt\"]:\n",
    "    dataframe[c] = dataframe[c].replace(to_replace=99999999, value=numpy.nan)\n",
    "    \n",
    "dataframe.describe().round()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb26bd06",
   "metadata": {},
   "source": [
    "Check to see if any of the loans are in an ambigous or unknown state. Turns out there's just one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c194a7a0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dataframe.status.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40863571",
   "metadata": {},
   "source": [
    "Remove the \"unk\" (unknown) values from the status, since we don't know whether or not they paid the loan back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ba5d59a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = dataframe[dataframe.status != \"unk\"]\n",
    "\n",
    "dataframe.status.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e55e70",
   "metadata": {},
   "source": [
    "Next we'll split the dataset into train, validation, and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9b085ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df_train_full, df_test = train_test_split(dataframe, test_size=0.2, random_state=11)\n",
    "\n",
    "df_train, df_validation = train_test_split(df_train_full, test_size=0.25, random_state=11)\n",
    "\n",
    "len(df_train), len(df_validation), len(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a62241",
   "metadata": {},
   "source": [
    "`status` is our target variable and we can call it `y` since it's our dependent variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6386c8b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get only the \"positive\" answers for the training and validation sets\n",
    "y_train = (df_train.status == \"default\").values\n",
    "y_validation = (df_validation.status == \"default\")\n",
    "\n",
    "# remove the 'status' column from the datasets so it doesn't impact our training\n",
    "del df_train[\"status\"]\n",
    "del df_validation[\"status\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "accbe7a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace the NaN values with 0\n",
    "df_train = df_train.fillna(0)\n",
    "df_validation = df_validation.fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0b0d091",
   "metadata": {},
   "source": [
    "We need to utilize one-hot encoding, where a value of `1` is \"hot\" and a value of `0` is \"cold\" using the `DictVectorizer` from SciKit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a0828804",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the dataframes into a list of dictionaries\n",
    "dict_train = df_train.to_dict(orient=\"records\")\n",
    "dict_validation = df_validation.to_dict(orient=\"records\")\n",
    "\n",
    "# pass the dictionaries into the DictVectorizer\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "dictVectorizer = DictVectorizer(sparse=False)\n",
    "\n",
    "X_train = dictVectorizer.fit_transform(dict_train)\n",
    "X_val = dictVectorizer.transform(dict_validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "158314bb",
   "metadata": {},
   "source": [
    "A decision tree is simply a collection of `if..else` statements that lead to final conclusions. It is similar in structure to a binary tree where the leaves are the resulting answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "355da190",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the scikit tree classifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# train the model using the `fit` method\n",
    "\n",
    "decision_tree = DecisionTreeClassifier()\n",
    "\n",
    "decision_tree.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "13038372",
   "metadata": {},
   "outputs": [],
   "source": [
    "# user scikit to get the AUC score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# use the decision tree to predict with the training set\n",
    "y_pred = decision_tree.predict_proba(X_train)[:,1]\n",
    "\n",
    "# get the score of actual vs predicted\n",
    "roc_auc_score(y_train, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b07bbf",
   "metadata": {},
   "source": [
    "`1.0` represents a perfect score, which is \"great\". We need to also check with the validation set as well, however."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4cae0469",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = decision_tree.predict_proba(X_val)[:,1]\n",
    "roc_auc_score(y_validation, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4bcfc39",
   "metadata": {},
   "source": [
    "The validation set only got a 66%. Meaning the model cannot \"generalize\" because it does not apply well to sets besides the set it trained on. This can happen when the model is too specific and \"memorizes\" the data.\n",
    "\n",
    "We can solve this by enforcing a level of simplicity on the model, so it cannot just match to the set perfectly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "04935bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prevent the tree from growing too large using max_depth\n",
    "\n",
    "decision_tree = DecisionTreeClassifier(max_depth=2)\n",
    "decision_tree.fit(X_train, y_train)\n",
    "\n",
    "# visualize the tree using an export function\n",
    "from sklearn.tree import export_text\n",
    "text_tree = export_text(decision_tree, feature_names=dictVectorizer.feature_names_)\n",
    "\n",
    "print(text_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "24b69cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the score with the new limited tree\n",
    "y_pred = decision_tree.predict_proba(X_train)[:,1]\n",
    "auc_train = roc_auc_score(y_train, y_pred)\n",
    "print(\"train auc\", auc_train)\n",
    "\n",
    "y_pred = decision_tree.predict_proba(X_val)[:,1]\n",
    "auc_validation = roc_auc_score(y_validation, y_pred)\n",
    "print(\"validation auc\", auc_validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d871458f",
   "metadata": {},
   "source": [
    "Worse score on the training set and only a slightly better score on the validation set, which indicates the model is now more generalized, rather than specific to a single set of data.\n",
    "\n",
    "However, there are other parameters besides `max_depth` that we can tune:\n",
    "<https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html>.\n",
    "\n",
    "We can use a loop to find the best `max_depth` from a list of potential candidates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f0b37dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "for depth in [1, 2, 3, 4, 5, 6, 10, 15, 20, None]:\n",
    "    decision_tree = DecisionTreeClassifier(max_depth=depth)\n",
    "    decision_tree.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = decision_tree.predict_proba(X_val)[:,1]\n",
    "    auc = roc_auc_score(y_validation, y_pred)\n",
    "    \n",
    "    print(\"%4s -> %.3f\" % (depth, auc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "811ffc36",
   "metadata": {},
   "source": [
    "With this loop, we can see that the optimal values are `4`, `5`, and `6`.\n",
    "\n",
    "We can then us another loop to figure out the best `min_samples_leaf` value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5c867835",
   "metadata": {},
   "outputs": [],
   "source": [
    "for depth in [4, 5, 6]:\n",
    "    print(\"depth\", depth)\n",
    "    \n",
    "    for leaf_size in [1, 5, 10, 15, 20, 50, 100, 200]:\n",
    "        decision_tree = DecisionTreeClassifier(max_depth=depth, min_samples_leaf=leaf_size)\n",
    "        decision_tree.fit(X_train, y_train)\n",
    "        \n",
    "        y_pred = decision_tree.predict_proba(X_val)[:,1]\n",
    "        \n",
    "        auc = roc_auc_score(y_validation, y_pred)\n",
    "        \n",
    "        print(\"%s -> %.3f\" % (leaf_size, auc))\n",
    "    \n",
    "    print() # line break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e340fe1",
   "metadata": {},
   "source": [
    "The best combination turns out to be `min_depth=6` and `min_samples_leaf=15`. We can now use them to train the model again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6e2edc5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_tree = DecisionTreeClassifier(max_depth=6, min_samples_leaf=15)\n",
    "decision_tree.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c7d41b3",
   "metadata": {},
   "source": [
    "## Random Forest and Ensemble Learning\n",
    "\n",
    "Combining the predictions of multiple, differnt models is called \"ensemble learning\". The results of all the models' predictions can be aggregated and used to form a final prediction which is often more accurate than a single prediction.\n",
    "\n",
    "You can train models on different features (e.g. `assets`, `debts`, `records` in the current scenario)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "28748cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the random forest utility from SciKit\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# n_estimators is the number of trees\n",
    "rand_forest = RandomForestClassifier(n_estimators=10)\n",
    "rand_forest.fit(X_train, y_train)\n",
    "\n",
    "# test out the training and see if it worked\n",
    "y_pred = rand_forest.predict_proba(X_val)[:,1]\n",
    "roc_auc_score(y_validation, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43363a99",
   "metadata": {},
   "source": [
    "The answer shown above will be more or less random each time you train the forest since the library utilizes a randomizer. You can utilize a random seed in order to get consistent results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8d0548a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the forest with a random seed\n",
    "rand_forest = RandomForestClassifier(n_estimators=10, random_state=3)\n",
    "rand_forest.fit(X_train, y_train)\n",
    "\n",
    "# test the training again\n",
    "y_pred = rand_forest.predict_proba(X_val)[:,1]\n",
    "roc_auc_score(y_validation, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9755f4a3",
   "metadata": {},
   "source": [
    "We can again use loops in order to tune parameters.\n",
    "\n",
    "We'll tune the `n_estimators` parameter this way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "98b4581b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "aucs = [] # create list of results\n",
    "\n",
    "for n in range(10, 201, 10): # 10 to 201 in steps of 10 (10, 20, 30, ..., 190, 200)\n",
    "    random_forest = RandomForestClassifier(n_estimators=n, random_state=3)\n",
    "    random_forest.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = random_forest.predict_proba(X_val)[:,1]\n",
    "    auc = roc_auc_score(y_validation, y_pred)\n",
    "    \n",
    "    print(\"%s -> %.3f\" % (n, auc))\n",
    "    \n",
    "    aucs.append(auc)\n",
    "    \n",
    "# plot the result to see the best auc visually\n",
    "pyplot.plot(range(10, 201, 10), aucs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "655d7b3a",
   "metadata": {},
   "source": [
    "Naturally, the random forest has additional parameters we can tune: <https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html>.\n",
    "\n",
    "We will now tune `max_depth` first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c246be",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_aucs = {} # dictionary of results\n",
    "\n",
    "for depth in [5, 10, 15]:\n",
    "    print(\"depth\", depth)\n",
    "    \n",
    "    aucs = []\n",
    "    \n",
    "    for n in range(10, 201, 10):\n",
    "        random_forest = RandomForestClassifier(n_estimators=n, max_depth=depth, random_state=1)\n",
    "        random_forest.fit(X_train, y_train)\n",
    "        \n",
    "        y_pred = random_forest.predict_proba(X_val)[:,1]\n",
    "        \n",
    "        auc = roc_auc_score(y_validation, y_pred)\n",
    "        \n",
    "        print(\"%s -> %.3f\" % (n, auc))\n",
    "        \n",
    "        aucs.append(auc)\n",
    "    \n",
    "    all_aucs[depth] = aucs\n",
    "    \n",
    "    print() # line break    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4221649a",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_trees = list(range(10, 201, 10))\n",
    "\n",
    "pyplot.plot(num_trees, all_aucs[5], label=\"depth=5\")\n",
    "pyplot.plot(num_trees, all_aucs[10], label=\"depth=10\")\n",
    "pyplot.plot(num_trees, all_aucs[15], label=\"depth=15\")\n",
    "\n",
    "pyplot.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbfa31c4",
   "metadata": {},
   "source": [
    "We can see ideally a depth of `10` should be used. With that, we can now train the `min_samples_leaf` value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50510d5c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_aucs = {} # dictionary of results\n",
    "\n",
    "for leaf_size in [3, 5, 10]:\n",
    "    print(\"min samples\", leaf_size)\n",
    "    \n",
    "    aucs = []\n",
    "    \n",
    "    for n in range(10, 201, 10):\n",
    "        random_forest = RandomForestClassifier(n_estimators=n, max_depth=10, random_state=1, min_samples_leaf=leaf_size)\n",
    "        random_forest.fit(X_train, y_train)\n",
    "        \n",
    "        y_pred = random_forest.predict_proba(X_val)[:,1]\n",
    "        \n",
    "        auc = roc_auc_score(y_validation, y_pred)\n",
    "        \n",
    "        print(\"%s -> %.3f\" % (n, auc))\n",
    "        \n",
    "        aucs.append(auc)\n",
    "    \n",
    "    all_aucs[leaf_size] = aucs\n",
    "    \n",
    "    print() # line break    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65926dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_trees = list(range(10, 201, 10))\n",
    "\n",
    "pyplot.plot(num_trees, all_aucs[3], label=\"leaf_size=3\")\n",
    "pyplot.plot(num_trees, all_aucs[5], label=\"leaf_size=5\")\n",
    "pyplot.plot(num_trees, all_aucs[10], label=\"leaf_size=10\")\n",
    "\n",
    "pyplot.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2090171",
   "metadata": {},
   "source": [
    "Thus we can determine the \"optimal\" parameters for our forest are:\n",
    "\n",
    "- `max_depth = 10`\n",
    "- `min_samples_leaf = 5`\n",
    "- `n_estimators = 200`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf929ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can now train the optimzal model\n",
    "\n",
    "random_forest = RandomForestClassifier(n_estimators=200, max_depth=10, min_samples_leaf=5, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22960984",
   "metadata": {},
   "source": [
    "## Gradient boosting\n",
    "\n",
    "Random trees are exactly that, random, but you can combine models a little more eintelligently using \"boosting\". Gradient boosting is where each model is trained off of the previous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a51022d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install xgboost"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
